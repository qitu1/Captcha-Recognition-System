{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73e9e507",
   "metadata": {},
   "source": [
    "# Faster R-CNN for CAPTCHA Recognition\n",
    "\n",
    "End-to-end character detection and recognition.\n",
    "\n",
    "Training: Images + Bounding boxes -> Learn detection + recognition  \n",
    "Testing: Images only -> Character sequence\n",
    "\n",
    "Total Loss = RPN Loss (objectness + bbox regression) + Classifier Loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "425c4654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from torchvision.ops import RoIAlign, nms, box_iou\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "torch.manual_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c487a9e",
   "metadata": {},
   "source": [
    "## 1. Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7b6de714",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 7764, Test: 1943\n"
     ]
    }
   ],
   "source": [
    "CHARS = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "CHAR_TO_IDX = {c: i for i, c in enumerate(CHARS)}\n",
    "IDX_TO_CHAR = {i: c for i, c in enumerate(CHARS)}\n",
    "NUM_CLASSES = len(CHARS)\n",
    "\n",
    "class CaptchaDataset(Dataset):\n",
    "    def __init__(self, img_dir, label_dir=None, is_test=False):\n",
    "        self.img_dir = img_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.is_test = is_test\n",
    "        self.files = [f for f in os.listdir(img_dir) if f.endswith('.png')]\n",
    "        self.to_tensor = transforms.ToTensor()\n",
    "        self.normalize = transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_name = self.files[idx]\n",
    "        img = Image.open(os.path.join(self.img_dir, img_name)).convert('RGB')\n",
    "        w, h = img.size\n",
    "        text = img_name.split('-')[0]\n",
    "        \n",
    "        boxes, labels = [], []\n",
    "        if not self.is_test and self.label_dir:\n",
    "            label_file = os.path.join(self.label_dir, img_name.replace('.png', '.txt'))\n",
    "            with open(label_file, 'r') as f:\n",
    "                for i, line in enumerate(f):\n",
    "                    if line.strip() and i < len(text):\n",
    "                        _, cx, cy, bw, bh = map(float, line.strip().split())\n",
    "                        boxes.append([(cx-bw/2)*w, (cy-bh/2)*h, (cx+bw/2)*w, (cy+bh/2)*h])\n",
    "                        labels.append(CHAR_TO_IDX[text[i].lower()])\n",
    "        \n",
    "        img = self.normalize(self.to_tensor(img))\n",
    "        return {\n",
    "            'image': img,\n",
    "            'boxes': torch.tensor(boxes, dtype=torch.float32),\n",
    "            'labels': torch.tensor(labels, dtype=torch.long),\n",
    "            'text': text\n",
    "        }\n",
    "\n",
    "def collate_fn(batch):\n",
    "    max_h = max(item['image'].shape[1] for item in batch)\n",
    "    max_w = max(item['image'].shape[2] for item in batch)\n",
    "    \n",
    "    images = []\n",
    "    for item in batch:\n",
    "        img = item['image']\n",
    "        c, h, w = img.shape\n",
    "        padded = torch.zeros(c, max_h, max_w)\n",
    "        padded[:, :h, :w] = img\n",
    "        images.append(padded)\n",
    "    \n",
    "    return {\n",
    "        'images': torch.stack(images),\n",
    "        'boxes_list': [item['boxes'] for item in batch],\n",
    "        'labels_list': [item['labels'] for item in batch],\n",
    "        'texts': [item['text'] for item in batch],\n",
    "        'img_sizes': [(item['image'].shape[1], item['image'].shape[2]) for item in batch]\n",
    "    }\n",
    "\n",
    "train_dataset = CaptchaDataset('../dataset/train', '../Segmented_dataset/train_labels', is_test=False)\n",
    "test_dataset = CaptchaDataset('../dataset/test', label_dir=None, is_test=True)\n",
    "\n",
    "print(f'Train: {len(train_dataset)}, Test: {len(test_dataset)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ff7958",
   "metadata": {},
   "source": [
    "## 2. Helper Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2259c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def match_proposals_to_gt(proposals, gt_boxes, gt_labels, pos_th=0.5, neg_th=0.3):\n",
    "    if len(gt_boxes) == 0:\n",
    "        return torch.full((len(proposals),), NUM_CLASSES, dtype=torch.long, device=proposals.device), proposals.clone()\n",
    "    \n",
    "    ious = box_iou(proposals, gt_boxes)\n",
    "    max_iou, max_idx = ious.max(dim=1)\n",
    "    \n",
    "    matched_labels = torch.full((len(proposals),), -1, dtype=torch.long, device=proposals.device)\n",
    "    matched_labels[max_iou >= pos_th] = gt_labels[max_idx[max_iou >= pos_th]]\n",
    "    matched_labels[max_iou < neg_th] = NUM_CLASSES\n",
    "    \n",
    "    return matched_labels, gt_boxes[max_idx]\n",
    "\n",
    "def sample_proposals(proposals, labels, n=256, pos_frac=0.5):\n",
    "    pos_idx = torch.where((labels >= 0) & (labels < NUM_CLASSES))[0]\n",
    "    neg_idx = torch.where(labels == NUM_CLASSES)[0]\n",
    "    \n",
    "    n_pos = min(int(n * pos_frac), len(pos_idx))\n",
    "    n_neg = min(n - n_pos, len(neg_idx))\n",
    "    \n",
    "    sampled = []\n",
    "    if len(pos_idx) > 0:\n",
    "        sampled.append(pos_idx[torch.randperm(len(pos_idx), device=pos_idx.device)[:n_pos]])\n",
    "    if len(neg_idx) > 0:\n",
    "        sampled.append(neg_idx[torch.randperm(len(neg_idx), device=neg_idx.device)[:n_neg]])\n",
    "    \n",
    "    return torch.cat(sampled) if sampled else torch.tensor([], dtype=torch.long, device=labels.device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d094ee",
   "metadata": {},
   "source": [
    "## 3. RPN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19fc04f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    def __init__(self, in_ch=512, n_anchors=2):\n",
    "        super().__init__()\n",
    "        self.anchor_sizes = [(30, 50), (40, 60)]\n",
    "        self.conv = nn.Conv2d(in_ch, 512, 3, padding=1)\n",
    "        self.cls = nn.Conv2d(512, n_anchors * 2, 1)\n",
    "        self.reg = nn.Conv2d(512, n_anchors * 4, 1)\n",
    "    \n",
    "    def generate_anchors(self, fm_size, img_size, device):\n",
    "        fh, fw = fm_size\n",
    "        img_h, img_w = img_size\n",
    "        stride_h, stride_w = img_h / fh, img_w / fw\n",
    "        \n",
    "        anchors = []\n",
    "        for i in range(fh):\n",
    "            for j in range(fw):\n",
    "                cx, cy = (j + 0.5) * stride_w, (i + 0.5) * stride_h\n",
    "                for aw, ah in self.anchor_sizes:\n",
    "                    anchors.append([cx-aw/2, cy-ah/2, cx+aw/2, cy+ah/2])\n",
    "        return torch.tensor(anchors, dtype=torch.float32, device=device)\n",
    "    \n",
    "    def forward(self, x, img_sizes):\n",
    "        B, _, H, W = x.shape\n",
    "        x = F.relu(self.conv(x))\n",
    "        \n",
    "        obj = self.cls(x).permute(0, 2, 3, 1).reshape(B, -1, 2)\n",
    "        deltas = self.reg(x).permute(0, 2, 3, 1).reshape(B, -1, 4)\n",
    "        \n",
    "        anchors_list = [self.generate_anchors((H, W), sz, x.device) for sz in img_sizes]\n",
    "        return obj, deltas, anchors_list\n",
    "    \n",
    "    def apply_deltas(self, anchors, deltas):\n",
    "        w = anchors[:, 2] - anchors[:, 0]\n",
    "        h = anchors[:, 3] - anchors[:, 1]\n",
    "        cx = anchors[:, 0] + 0.5 * w\n",
    "        cy = anchors[:, 1] + 0.5 * h\n",
    "        \n",
    "        dx, dy, dw, dh = deltas.unbind(dim=1)\n",
    "        pred_cx = dx * w + cx\n",
    "        pred_cy = dy * h + cy\n",
    "        pred_w = torch.exp(dw.clamp(max=4)) * w\n",
    "        pred_h = torch.exp(dh.clamp(max=4)) * h\n",
    "        \n",
    "        return torch.stack([pred_cx-0.5*pred_w, pred_cy-0.5*pred_h, \n",
    "                           pred_cx+0.5*pred_w, pred_cy+0.5*pred_h], dim=1)\n",
    "    \n",
    "    def compute_targets(self, anchors, gt_boxes):\n",
    "        aw = anchors[:, 2] - anchors[:, 0]\n",
    "        ah = anchors[:, 3] - anchors[:, 1]\n",
    "        acx = anchors[:, 0] + 0.5 * aw\n",
    "        acy = anchors[:, 1] + 0.5 * ah\n",
    "        \n",
    "        gw = gt_boxes[:, 2] - gt_boxes[:, 0]\n",
    "        gh = gt_boxes[:, 3] - gt_boxes[:, 1]\n",
    "        gcx = gt_boxes[:, 0] + 0.5 * gw\n",
    "        gcy = gt_boxes[:, 1] + 0.5 * gh\n",
    "        \n",
    "        return torch.stack([(gcx-acx)/aw, (gcy-acy)/ah, \n",
    "                           torch.log(gw/aw), torch.log(gh/ah)], dim=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de46be9",
   "metadata": {},
   "source": [
    "## 4. Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f8b8149c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters: 33,105,201\n"
     ]
    }
   ],
   "source": [
    "class FasterRCNN(nn.Module):\n",
    "    def __init__(self, num_classes=36):\n",
    "        super().__init__()\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.backbone = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 3, padding=1), nn.BatchNorm2d(64), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(64, 128, 3, padding=1), nn.BatchNorm2d(128), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, 3, padding=1), nn.BatchNorm2d(256), nn.ReLU(), nn.MaxPool2d(2),\n",
    "            nn.Conv2d(256, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, 3, padding=1), nn.BatchNorm2d(512), nn.ReLU()\n",
    "        )\n",
    "        \n",
    "        self.rpn = RPN(512, 2)\n",
    "        self.roi_align = RoIAlign((7, 7), spatial_scale=1/8, sampling_ratio=2)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 1024), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(1024, 512), nn.ReLU(), nn.Dropout(0.5),\n",
    "            nn.Linear(512, num_classes + 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, images, img_sizes, gt_boxes_list=None, gt_labels_list=None, mode='train'):\n",
    "        features = self.backbone(images)\n",
    "        obj, deltas, anchors_list = self.rpn(features, img_sizes)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            return self._train(features, obj, deltas, anchors_list, \n",
    "                             gt_boxes_list, gt_labels_list, img_sizes)\n",
    "        else:\n",
    "            return self._test(features, obj, deltas, anchors_list, img_sizes)\n",
    "    \n",
    "    def _train(self, feat, obj, deltas, anchors_list, gt_boxes_list, gt_labels_list, img_sizes):\n",
    "        B = len(anchors_list)\n",
    "        rpn_cls_loss = rpn_reg_loss = 0\n",
    "        all_props, all_labels, all_bidx = [], [], []\n",
    "        \n",
    "        for i in range(B):\n",
    "            anchors = anchors_list[i]\n",
    "            gt_boxes = gt_boxes_list[i]\n",
    "            gt_labels = gt_labels_list[i]\n",
    "            \n",
    "            props = self.rpn.apply_deltas(anchors, deltas[i])\n",
    "            h, w = img_sizes[i]\n",
    "            props[:, 0::2].clamp_(0, w)\n",
    "            props[:, 1::2].clamp_(0, h)\n",
    "            \n",
    "            if len(gt_boxes) > 0:\n",
    "                # RPN loss\n",
    "                anc_labels, matched_gt = match_proposals_to_gt(anchors, gt_boxes, gt_labels, 0.7, 0.3)\n",
    "                valid = anc_labels >= 0\n",
    "                if valid.sum() > 0:\n",
    "                    rpn_cls_loss += F.cross_entropy(obj[i][valid], \n",
    "                                                    (anc_labels[valid] < self.num_classes).long())\n",
    "                \n",
    "                pos = (anc_labels >= 0) & (anc_labels < self.num_classes)\n",
    "                if pos.sum() > 0:\n",
    "                    targets = self.rpn.compute_targets(anchors[pos], matched_gt[pos])\n",
    "                    rpn_reg_loss += F.smooth_l1_loss(deltas[i][pos], targets)\n",
    "                \n",
    "                # Classifier samples\n",
    "                prop_labels, _ = match_proposals_to_gt(props, gt_boxes, gt_labels, 0.5, 0.3)\n",
    "                sampled = sample_proposals(props, prop_labels, 128)\n",
    "                \n",
    "                if len(sampled) > 0:\n",
    "                    all_props.append(props[sampled])\n",
    "                    all_labels.append(prop_labels[sampled])\n",
    "                    all_bidx.extend([i] * len(sampled))\n",
    "        \n",
    "        cls_loss = 0\n",
    "        if all_props:\n",
    "            all_props = torch.cat(all_props)\n",
    "            all_labels = torch.cat(all_labels)\n",
    "            all_bidx = torch.tensor(all_bidx, dtype=torch.float32, device=feat.device)\n",
    "            \n",
    "            roi_boxes = torch.cat([all_bidx.unsqueeze(1), all_props], dim=1)\n",
    "            roi_feat = self.roi_align(feat, roi_boxes).view(len(all_props), -1)\n",
    "            preds = self.classifier(roi_feat)\n",
    "            cls_loss = F.cross_entropy(preds, all_labels)\n",
    "        \n",
    "        return {\n",
    "            'rpn_cls': rpn_cls_loss / B,\n",
    "            'rpn_reg': rpn_reg_loss / B,\n",
    "            'cls': cls_loss\n",
    "        }\n",
    "    \n",
    "    def _test(self, feat, obj, deltas, anchors_list, img_sizes):\n",
    "        batch_props = []\n",
    "        \n",
    "        for i in range(len(anchors_list)):\n",
    "            scores = F.softmax(obj[i], dim=1)[:, 1]\n",
    "            props = self.rpn.apply_deltas(anchors_list[i], deltas[i])\n",
    "            \n",
    "            h, w = img_sizes[i]\n",
    "            props[:, 0::2].clamp_(0, w)\n",
    "            props[:, 1::2].clamp_(0, h)\n",
    "            \n",
    "            keep = scores > 0.5\n",
    "            props, scores = props[keep], scores[keep]\n",
    "            \n",
    "            if len(props) > 0:\n",
    "                keep_nms = nms(props, scores, 0.3)\n",
    "                props = props[keep_nms[:10]]\n",
    "            \n",
    "            batch_props.append(props)\n",
    "        \n",
    "        roi_boxes = []\n",
    "        for bi, props in enumerate(batch_props):\n",
    "            if len(props) > 0:\n",
    "                bidx = torch.full((len(props), 1), bi, dtype=torch.float32, device=props.device)\n",
    "                roi_boxes.append(torch.cat([bidx, props], dim=1))\n",
    "        \n",
    "        if not roi_boxes:\n",
    "            return [], []\n",
    "        \n",
    "        roi_boxes = torch.cat(roi_boxes)\n",
    "        roi_feat = self.roi_align(feat, roi_boxes).view(len(roi_boxes), -1)\n",
    "        preds = self.classifier(roi_feat)\n",
    "        \n",
    "        boxes_list, preds_list = [], []\n",
    "        start = 0\n",
    "        for props in batch_props:\n",
    "            n = len(props)\n",
    "            if n > 0:\n",
    "                preds_list.append(preds[start:start+n])\n",
    "                boxes_list.append(props)\n",
    "                start += n\n",
    "            else:\n",
    "                preds_list.append(torch.empty(0, self.num_classes+1, device=feat.device))\n",
    "                boxes_list.append(torch.empty(0, 4, device=feat.device))\n",
    "        \n",
    "        return boxes_list, preds_list\n",
    "\n",
    "model = FasterRCNN(NUM_CLASSES).to(device)\n",
    "print(f'Parameters: {sum(p.numel() for p in model.parameters()):,}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856e0705",
   "metadata": {},
   "source": [
    "## 5. Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "867a5195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready to train\n"
     ]
    }
   ],
   "source": [
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "def train_epoch(model, loader, optimizer):\n",
    "    model.train()\n",
    "    total = rpn_c = rpn_r = cls = 0\n",
    "    \n",
    "    for batch in tqdm(loader, desc='Training'):\n",
    "        images = batch['images'].to(device)\n",
    "        boxes = [b.to(device) for b in batch['boxes_list']]\n",
    "        labels = [l.to(device) for l in batch['labels_list']]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        losses = model(images, batch['img_sizes'], boxes, labels, 'train')\n",
    "        \n",
    "        loss = losses['rpn_cls'] + losses['rpn_reg'] + losses['cls']\n",
    "        if loss > 0:\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "            optimizer.step()\n",
    "            \n",
    "            total += loss.item() if torch.is_tensor(loss) else loss\n",
    "            rpn_c += losses['rpn_cls'].item() if torch.is_tensor(losses['rpn_cls']) else losses['rpn_cls']\n",
    "            rpn_r += losses['rpn_reg'].item() if torch.is_tensor(losses['rpn_reg']) else losses['rpn_reg']\n",
    "            cls += losses['cls'].item() if torch.is_tensor(losses['cls']) else losses['cls']\n",
    "    \n",
    "    n = len(loader)\n",
    "    return {'total': total/n, 'rpn_cls': rpn_c/n, 'rpn_reg': rpn_r/n, 'cls': cls/n}\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(loader, desc='Eval'):\n",
    "            images = batch['images'].to(device)\n",
    "            boxes_list, preds_list = model(images, batch['img_sizes'], mode='test')\n",
    "            \n",
    "            for boxes, preds, text in zip(boxes_list, preds_list, batch['texts']):\n",
    "                if len(preds) > 0:\n",
    "                    pred_cls = preds.argmax(1)\n",
    "                    valid = pred_cls < NUM_CLASSES\n",
    "                    \n",
    "                    if valid.sum() > 0:\n",
    "                        x = boxes[valid][:, 0].cpu().numpy()\n",
    "                        sorted_idx = np.argsort(x)\n",
    "                        pred_text = ''.join([IDX_TO_CHAR[pred_cls[valid][i].item()] \n",
    "                                           for i in sorted_idx])\n",
    "                        if pred_text.lower() == text.lower():\n",
    "                            correct += 1\n",
    "                total += 1\n",
    "    \n",
    "    return 100 * correct / total if total > 0 else 0\n",
    "\n",
    "print('Ready to train')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb7fc77",
   "metadata": {},
   "source": [
    "## 6. Run Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5e911a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  31%|███       | 601/1941 [27:17<1:00:50,  2.72s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_EPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     losses = \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mLoss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m\"\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m (RPN_cls:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m\"\u001b[39m\u001b[33mrpn_cls\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      8\u001b[39m           \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mRPN_reg:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m\"\u001b[39m\u001b[33mrpn_reg\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Cls:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlosses[\u001b[33m\"\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m\"\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     10\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (epoch + \u001b[32m1\u001b[39m) % \u001b[32m5\u001b[39m == \u001b[32m0\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 17\u001b[39m, in \u001b[36mtrain_epoch\u001b[39m\u001b[34m(model, loader, optimizer)\u001b[39m\n\u001b[32m     14\u001b[39m labels = [l.to(device) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[33m'\u001b[39m\u001b[33mlabels_list\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     16\u001b[39m optimizer.zero_grad()\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m losses = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mimg_sizes\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mboxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     19\u001b[39m loss = losses[\u001b[33m'\u001b[39m\u001b[33mrpn_cls\u001b[39m\u001b[33m'\u001b[39m] + losses[\u001b[33m'\u001b[39m\u001b[33mrpn_reg\u001b[39m\u001b[33m'\u001b[39m] + losses[\u001b[33m'\u001b[39m\u001b[33mcls\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m loss > \u001b[32m0\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\software\\Anaconda\\envs\\CS4243_lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\software\\Anaconda\\envs\\CS4243_lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 25\u001b[39m, in \u001b[36mFasterRCNN.forward\u001b[39m\u001b[34m(self, images, img_sizes, gt_boxes_list, gt_labels_list, mode)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, images, img_sizes, gt_boxes_list=\u001b[38;5;28;01mNone\u001b[39;00m, gt_labels_list=\u001b[38;5;28;01mNone\u001b[39;00m, mode=\u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m     features = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m     obj, deltas, anchors_list = \u001b[38;5;28mself\u001b[39m.rpn(features, img_sizes)\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m mode == \u001b[33m'\u001b[39m\u001b[33mtrain\u001b[39m\u001b[33m'\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\software\\Anaconda\\envs\\CS4243_lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\software\\Anaconda\\envs\\CS4243_lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\software\\Anaconda\\envs\\CS4243_lab\\Lib\\site-packages\\torch\\nn\\modules\\container.py:250\u001b[39m, in \u001b[36mSequential.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    246\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    247\u001b[39m \u001b[33;03mRuns the forward pass.\u001b[39;00m\n\u001b[32m    248\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     \u001b[38;5;28minput\u001b[39m = \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    251\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\software\\Anaconda\\envs\\CS4243_lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\software\\Anaconda\\envs\\CS4243_lab\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\software\\Anaconda\\envs\\CS4243_lab\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:193\u001b[39m, in \u001b[36m_BatchNorm.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    186\u001b[39m     bn_training = (\u001b[38;5;28mself\u001b[39m.running_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m.running_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    188\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[33;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[33;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[32m    192\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m193\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[32m    196\u001b[39m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_mean\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[32m    200\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\software\\Anaconda\\envs\\CS4243_lab\\Lib\\site-packages\\torch\\nn\\functional.py:2813\u001b[39m, in \u001b[36mbatch_norm\u001b[39m\u001b[34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[39m\n\u001b[32m   2810\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[32m   2811\u001b[39m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m.size())\n\u001b[32m-> \u001b[39m\u001b[32m2813\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2814\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2815\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2816\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2817\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2818\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2819\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2820\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2821\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2822\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackends\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcudnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2823\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 20\n",
    "best_acc = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    print(f'\\nEpoch {epoch+1}/{NUM_EPOCHS}')\n",
    "    losses = train_epoch(model, train_loader, optimizer)\n",
    "    print(f'Loss: {losses[\"total\"]:.4f} (RPN_cls:{losses[\"rpn_cls\"]:.4f} '\n",
    "          f'RPN_reg:{losses[\"rpn_reg\"]:.4f} Cls:{losses[\"cls\"]:.4f})')\n",
    "    \n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        acc = evaluate(model, test_loader)\n",
    "        print(f'Test Acc: {acc:.2f}%')\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            torch.save(model.state_dict(), 'faster_rcnn.pth')\n",
    "            print('Saved')\n",
    "    \n",
    "    scheduler.step()\n",
    "\n",
    "print(f'\\nBest: {best_acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf0ec14",
   "metadata": {},
   "source": [
    "## 7. Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef25d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('faster_rcnn.pth'))\n",
    "acc = evaluate(model, test_loader)\n",
    "print(f'\\nTest Accuracy: {acc:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454d07eb",
   "metadata": {},
   "source": [
    "## 8. Visualization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d846c76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(model, dataset, idx):\n",
    "    model.eval()\n",
    "    sample = dataset[idx]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        img = sample['image'].unsqueeze(0).to(device)\n",
    "        h, w = sample['image'].shape[1], sample['image'].shape[2]\n",
    "        boxes_list, preds_list = model(img, [(h, w)], mode='test')\n",
    "    \n",
    "    boxes = boxes_list[0].cpu() if len(boxes_list[0]) > 0 else torch.empty(0, 4)\n",
    "    preds = preds_list[0]\n",
    "    \n",
    "    # Denormalize\n",
    "    img = sample['image']\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)\n",
    "    img = torch.clamp(img * std + mean, 0, 1).permute(1, 2, 0).numpy()\n",
    "    \n",
    "    # Get text\n",
    "    pred_text = ''\n",
    "    valid_boxes = boxes\n",
    "    if len(preds) > 0:\n",
    "        pred_cls = preds.argmax(1)\n",
    "        valid = pred_cls < NUM_CLASSES\n",
    "        if valid.sum() > 0:\n",
    "            valid_boxes = boxes[valid]\n",
    "            x = valid_boxes[:, 0].numpy()\n",
    "            sorted_idx = np.argsort(x)\n",
    "            pred_text = ''.join([IDX_TO_CHAR[pred_cls[valid][i].item()] for i in sorted_idx])\n",
    "        else:\n",
    "            pred_text = '(bg)'\n",
    "    else:\n",
    "        pred_text = '(none)'\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(12, 3))\n",
    "    ax.imshow(img)\n",
    "    for box in valid_boxes.numpy():\n",
    "        x1, y1, x2, y2 = box\n",
    "        rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, \n",
    "                                fill=False, edgecolor='red', linewidth=2)\n",
    "        ax.add_patch(rect)\n",
    "    \n",
    "    correct = pred_text.lower() == sample['text'].lower()\n",
    "    color = 'green' if correct else 'red'\n",
    "    ax.set_title(f'GT: {sample[\"text\"]} | Pred: {pred_text} | {\"OK\" if correct else \"WRONG\"}',\n",
    "                fontsize=11, color=color, fontweight='bold')\n",
    "    ax.axis('off')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "for i in range(min(10, len(test_dataset))):\n",
    "    visualize(model, test_dataset, i)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CS4243_lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
